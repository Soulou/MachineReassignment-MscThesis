\chapter{Container load balancing in cloud environment}
\label{mainidea}
\lhead{Chapter 2. \emph{Container load balancing in cloud environment}}

\section{Definition}

Operating system-level virtualization has been existing for a long time, the
premises of this technology can be found at the early times of computing in the
1960s. It wasn't named the same way, but companies like \textbf{IBM} had to
allow users to split the computing time of a mainframe for example. Different
operating system weren't started, but processes were strongly isolated inside
one unique OS.

Operating system-level virtualisation was not common in most infrastructures
until the nineties where common open-source kernels started to implement
features related to it. \textbf{Jails} appeared first in the BSD kernel (1998).
Then, Sun developed Solaris (Sun UNIX operating system) \textbf{zones} in 2005,
the same year as the \textbf{OpenVZ} implementation over the Linux kernel. But
tools which really popularized this concept to a broader audience is named
\textbf{LXC} for LinuX Containers, introduced in 2009.

Containers are running over the same operating system as the host server, they
are sharing the same drivers, but all the processes contained in them are
running in an isolated environment. The memory consumption, the CPU usage, the
network and disk IO are monitored and managed by these container engines
sending the corresponding instructions to their respective kernel. 

This technology has been more and more used in the industry these last 3-5
years, more and more companies are adopting it. It may be to offer services for
companies like OpenShift (Red Hat), Cloud Foundry, Heroku, MongoLab, etc\@. or to
manage the hosting of their own projects: Google, Ebay, Spotify. What kind of
applications are containerized? Any software is able to run in a container, the
work done by~\citet*{hpccontainers} shows in the field of HPC, containers are
mature enough to replace virtual machines and get better performance.

This is a completely different approach to process isolation compare to
classical virtual machines where hypervisors and VMs have been following the
paradigm where everything is virtualized, creating overhead and slower
performance, then optimization is added afterward.

The main idea behind containers is, based on the host operating system, only
the required devices/features will be virtualized, and finally the level of
performance is close to native efficiency. Already in 2007, some articles have
shown the possibility to use container-based virtualization instead of
hypervisors and virtual machines as a high-performance
alternative~\cite{containersAsAlternative}.

\begin{figure}
	\includegraphics[width=\textwidth]{./Images/containers_vs_vms.png}
	\caption{Structural difference between containers and VMs}
\end{figure}

\section{Docker container engine}

\subsection{A bit of history}

\textbf{Docker} is a recent project. Introduced in May 2013 by the Platform as
a Service provider \textbf{Dotcloud} with the ambition to create a standard way
to manage multiplatform containers, \textbf{Docker} has rapidly been promoted
as a mainstream project supported by all the main tech companies.  Their
catchword is "Build once, run everywhere", but this is only a half truth.

The project has been created as a REST-ish API server, using LXC tools to
manage the containers themselves, however LXC is, as its name shows (LinuX
Container) specific to Linux-based operating systems, at that time it was even
more restricted, \textbf{Docker} was only working on Ubuntu Linux amd64. This
is why the "run anywhere" was a bit biased at that time. One year later, the
project has abandoned LXC to create their own library named
\textit{libcontainer}, which is able to run on mostly any Linux. In the future,
the project leaders want docker to be able to run on any kind of
containerization system : BSD Jails, Solaris Zones, and Linux Containers, to
become the real interoperable standard for containerization.

\subsection{Its contribution to operating system level virtualization}

As stated previously, containers were existing for a long time before
\textbf{docker}, but this project has succeeded to create a wave of motivation
and keen interest around this technology. People were so enthusiastic that
\textbf{Dotcloud} pivoted their activity, to focus on \textbf{docker} and
changed its name to \textbf{Docker inc.}

What has been brought by this container engine is a simple way to manage and
deploy containers one a large amount of server through a simple API. This
simplification took over the LXC tools which were known to be difficult to
handle.

\textbf{Docker} also uses copy on write filesystems, it means that if different
applications are using the same base file system, it only needs to be present once.
This precise characteristic let users create containers in milliseconds as there
is no file to copy.

Currently, thanks to \textbf{Docker inc.} containers are really fashionable,
every tech companies is looking at them and their evolution and more and more
people try to get rid of heavy virtual machines, because even if containers do
not have all the features of virtual machines, they are good enough in a lot of
cases.

\section{Advantages}

Cloud providers are looking for the best way to setup a multi-tenant
architecture which is secure enough and fast enough. "Containers" is an answer
to this issue. For example, the company \textbf{MongoLab} is hosting thousands
of MongoDB databases. Data is something critical for any company, so
\textbf{MongoLab} needs to isolate each instance of MongoDB from each other.
We can assume that most of the databases they are hosting don't have a really
high traffic. Having a virtual machine for each of those instances is clearly
something oversized and would result on high provisioning overhead (duration of
virtual machine boot), storage overhead (1 full operating system per instance),
etc.  This company is using containers because, it allows them to isolate the
databases, to provision them instantly, and the files required to run MongoDB
are only present once on their servers (physical or virtual).

The interface proposed by \textbf{Docker} is easy to manipulate and is using
standard technologies. There is no need to use a custom protocol to communicate
with it, everything is transiting through HTTP\@. There is now an easy way to
isolate applications. Compared to virtual machines, the usage difficulty level
has been highly decreased.

\section{Limits}

Containers are not able to live-migrate from one host to another with a
standard linux kernel yet. This feature is possible with an \textbf{OpenVZ}
patched kernel because those patches implement the checkpoint/restore
operations for the containers, but for a vanilla Linux kernel, it does not
exist yet. Some developers/hackers are trying to clean the code of OpenVZ and
push the features to the mainstream kernel with the \cite{websiteCRIU} project,
but so far the results are mostly drafty and unstable.

This main limit results in the difficulty to host stateful applications like a
database. It can be isolated in a container but we don't have the possibility
to move it without any downtime, the container has to be stop first then
restarted on another host. This is particularly blocking in the case of
production environment where every downtime leads to money loses for instance.

\section{Web Application}

As containerized stateful applications can not be cleanly load balanced among a
set of servers (a downtime is required), stateless web applications will be
targeted, as stated in the introduction of this work.

A Web application is an applicative server which uses the web standards to
communicate with clients. There are two main types of web services. The
websites, which are rendering HTML/JS/CSS web pages to users, and web services
defining an API and answering with standard data formats like XML or JSON\@. Both
of them are using HTTP as transfer protocol.

By the nature of HTTP, web applications are mostly stateless. Each resource
request is done using a new connection (except the case of reusing opened
connections). When a web application is stateful it is linked to the
application itself which is linking information to a local session or
connection.

These last 5 years, more and more of the web services have been written based
on some or all the principles of the REST method which declares as "best
practice" to create complete stateless applications. Additionally, another
manifesto, the ``12 Factors~\cite{website12Factors}'', has become a standard
set of good practices for web development (website and web services)

The main advantage of stateless services is that they are able to scale
horizontally easily: the first step is to spawn new instances of the service,
and then modify the routing table of a frontal reverse-proxy. As a result the
requests will be distributed among all the instances.

\section{Application balancing in the infrastructure}

When a web application has to be moved from one host to another, there should
be no unavailability and the current requests have to stopped gracefully. To
solve the first issue, the following walkthrough has to be followed:

\begin{enumerate}
	\item{Create a new instance of the application - Instantiate a new
	container of a web application}
	\item{Wait until the instance is available - TCP ping the application
	until a connection is established}
	\item{Change reverse proxy routing to route requests to the new
	container and not the old one}
	\item{Stop the old container to free its resources}
\end{enumerate}

To solve the second issue, it should be handle by the application itself. When
the system is querying the old container to stop. It actually sends a signal to
it. In most systems (Systemd, Upstart at the system level, or Heroku and
Dotcloud at the PaaS level), SIGTERM is sent, then the application has some
time to shutdown. In the case where the application is still running a while
after receiving the signal, SIGKILL is sent to get rid of the process.

\section{Operations on containers}

\subsection{Load balancing and consolidation}

The load balancing process consists in moving applications in order avoid having
overloaded and underloaded hosts.

\begin{figure}[h!]
	\includegraphics[width=\textwidth]{./Images/loadbalancing.png}
	\caption{Schema of a load balancing process}
\end{figure}

The load balancing operation can be considered as a routine in order to avoid
overloaded and underloaded servers. It has to be run periodically in order to
move containers from overloaded host to more available hosts. But also to avoid
servers to be nearly empty, this is the process of consolidation. In this case
the server costs money and it would be better if the applications on the
underloaded servers could be moved.

\subsection{Resource Allocation}

When a new application has to start on the cluster, a container has to be
created.  At that step, it is required to find the best server to host this
newly spawned application

\begin{figure}[h!]
	\includegraphics[width=\textwidth]{./Images/resourceallocation.png}
	\caption{Schema of a resource allocation process}
\end{figure}

For that step, it is required to find the most available server, because
deploying an application on a node which is already under an important load can
have repercussions on all the different containers hosted on it.
