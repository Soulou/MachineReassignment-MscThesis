\chapter{Literature Review}
\label{litreview}
\lhead{Chapter 1. \emph{Literature Review}}

\section{Motivation}

Cloud computing is currently a major trend in the IT culture. Companies
are more and more using cloud infrastructure, even if their reasons are still
blurred and have been strongly altered by a skilled salesperson. However, behind
this marketing layer, there are some concrete technological evolutions and
advantages in cloud related products.

The legitimate question is “Why should people use cloud services? ”. Whether
it concerns virtual machines, whether it is linked to containers, the answers
are multiple. Valentina Salapura explains how a virtualized environment
improves the resiliency of an infrastructure~\cite{virtresiliency}.
As cloud infrastructure should be highly available and fault tolerant, in order
to provide a high-quality SLA (Service Level Agreement) and attract customers,
the overall quality of the hosting is in most of the cases better that what can
be done by the customers themselves. It is not their job, they don't have as
much knowleldge as the provider which is specialized in this field.

The \textbf{IaaS} provider owns the infrastructure, commonly composed by
several physical machines (PMs), sometimes located in different data centers.
Each of these PMs contains a variable number of virtual machines (VMs), rented
by the customers. In the scope of a \textbf{PaaS}, the provider has the control
on the way the users access those instances. Containers are often used to
isolate the users with each other. The interesting problematic concerns the
assignment of these VMs on the PMs or the containers on the VMs, what is the
optimal distributions of the containers among the different servers?

At the scope of the physical server, Thomas Setzer and Alexander Stage based
their study on the statement that energy represents up to 50\% of operating
costs of an infrastructure~\cite{reassignmentElectricitysaving}. That's why
there is a need to optimize it. For consumers of commercial \textit{IaaS}
offers, the main goal is to use the minimum number of virtual machines while
having enough resources for all the applications running on their current
infrastructure. They do not directly pay the electricity, it is included in the
price paid to the provider, the focus is on the level of performance directly.

\section{Algorithms}

Cloud computing is a hot topic in the IT industry.  New problematics have
appeared, the resource allocation problem is one of them. The different
algorithms listed in this document gather publications about resource
allocation or reassignment.

In the first part, a general solution method is studied. Using Linear
Programming, an optimized result is able to be found. However as this problem
is NP-hard, it is not possible to use Linear Programming anymore when the
amount of objects or the number of resources are getting big. This is why we
have to use heuristics. Heuristics decrease the quality of the results, but
allow the computation to be executed much more fastly.

Among those heuristic algorithms, we are going to have a special focus on the
bin packing heuristics which are the methods used later in this thesis. An
overview of other methods like the Ant Colony heuristics, network flows
based, or genetic algorithms will be proposed.

\subsection{Linear Programming}

Also known as Linear optimization. It is specialisation of mathematical
programming, which is focused on linear functions. The main goal of linear
programming is to find a maximum or a minimum to a linear function given a set
of constraints. In other words: maximizing profits while minimizing costs. In
the scope of resource allocation, it is required to define the different
variables, the function to optimize and the constraints linked to the
variables.

In the work \textit{Cost-Optimal Scheduling in Hybrid IaaS Clouds for Deadline
Constrained Workloads}~\cite{allocationHybrid}, the authors are working with
linear programming. The aim of their study is to define a way to optimize the
number of allocated virtual machines splitted in different cloud
infrastructures. Different constraints are defined to setup the scope of the
function to minimize.

\begin{figequation}
	\caption{Example of linear optimization problem}
	\[
		Minimize
		\sum_{k=1}^A \sum_{l=1}^{T_k} \sum_{i=1}^I \sum_{j=1}^C ( y_{klij} \cdot (ni_{kl} \cdot {pi_j} + no_{kl} \cdot po_j) + \sum_{s=1}^S ( p_{ij} \cdot x_{klijs} ))
	\]
\end{figequation}

\textit{Equation 1} is the problem they want to solve, in this case a cost
minimization problem. How can we minimize for each task $t$ of each application
$k$ in each virtual machine $i$ of each cloud infrastructure $j$ the price of
the input and output bandwidth ($ni \cdot pi_j$ and $no_{kl} \cdot po_j$) and
the price the requested virtual machines ($x_{klijs} \cdot p_{ij}$) at each
unit of time ($S$)

\begin{figequation}
	\caption{Example of constraints in a linear program}
	\[
		\forall j \in [1,C], s \in [1,S]:
		\sum_{k=1}^{A} \sum_{l=1}^{T_k} \sum_{i=1}^{I} cpu_i \cdot x_{klijs} \leq maxcpu_j
	\]
\end{figequation}

The \textit{Equation 2} defines a constraint from the linear problem, which
explains that in each cloud, at each unit of time, the sum of all the tasks running
on all the virtual machines instantiated should be less than the number of CPUs
available. (There is note that in the case of public clouds, the amount of CPU
is considered unlimited so this constraint becomes void).

\vspace{1em}

The work of M. Stillwell, F. Vivien, and H.
Casanova~\cite{allocationHeterogeneous}, which focuses virtual machine
resources allocation in heterogeneous environment, also starts by defining a
formal model based on linear programming. However, as explained in this
publication, resolving such a problem requires an exponential time, linked to
the amount of allocations to achieve. As a result using directly this solution
on an important workload is not feasible.

\vspace{1em}

Linear optimization relaxation can be used to simplify the original problem and
transform it from an exponential complexity to a polynomial
complexity~\citet*{mathsRrndlp}. The “random rounding” (RRND) is a
probabilistic approach which modifies some of the constraints by a weaker one.

\begin{figequation}
	\caption{Application of random rounding}
	\[
		\text{constraint before: } 0 \leq x \leq 1 \\
	\]
	\[
		\text{constraint after: } x_r \in {0, 1}
	\]
	\[
		x_r = 1 \text{ with a probability of $x$, otherwise: $0$}
	\]
\end{figequation}

However, the RRND approach is quickly discarded as the results are not good
enough in the case of resource allocations in heterogeneous environment.

\subsection{Bin packing}

Bin packing is one of the most common method to calculate resource allocation
or re-allocation. It consists in representing “bins” associated to a storage
capacity and “items” which have to be packed into those bins.

\subsubsection{Different variants}

Two main types of bin packing algorithm coexist. On the one hand, those
considered as “offline”. They consider that we have access to all the items to
find the optimal packing on the different bins. This problem is a NP-hard
problem, there is no, to this day, a polynomial way to solve this problem.
That is why to answer this problem in a reasonable duration, different
heuristics have to be defined in~\cite*{mathsBpheuristics}:

\vspace{1em}
\begin{center}
	\begin{tabular}{| l | p{7cm} |}
		\hline
		Algorithm Name & Description \\
		\hline
		First Fit (FF) & Pack the item in the first bin with a large enough capacity \\
		\hline
		Best Fit & Pack the item in the bin which will have the less capacity after packing \\
		\hline
		Worst Fit & Opposite of Best Fit: Pack the item in the bin with the biggest capacity \\
		\hline
		Next Fit & Same as FF except that instead of reconsidering the first bin after packing, the current one then the next one is considered \\
		\hline
		*-Fit Decreasing & First, sort the items in a decreasing order, then apply any of the *-Fit algorithm \\
		\hline
	\end{tabular}
\end{center}
\vspace{1em}

Those different algorithms reduce the complexity of the packing operation to
$O(n\log{n})$. But as~\cite{mathsBpheuristics} title explains: they are
“Near-Optimal”. The issue is finally to find the best ratio optimality/complexity.

On the other hand, the “online” algorithms, which, on the contrary, are packing
items at the time they are arriving. In this case bins are already partially
filled with other items, and it is not always possible to move those. Thus, the
main goal is to find the best assignment for the newly coming item. Previous
*-Fit could be directly used. However, it is really limited to pack one item in
a set of bin, this is why different algorithms have been developed

To answer more precisely to the cloud resource allocation problem, some people
have defined some variants of those two main categories of bin packing
algorithms. G. Gambosi and A. Postiglione and M. Talamo have developed a
“relaxed online bin packing” algorithm~\cite{mathsRelaxedonlinebp}. It may be
represented as a mix between online and offline bin packing. When a new item has
to be packed, it allows an additional limited number of moves among the
currently packed items.

Another interesting variant is the dynamic online bin packing defined by Joseph
Wun-Tan. It differentiates itself from standard online bin packing by allowing
items to be removed from bins. Static online bin packing does not allow these
items changes, once an item has been placed it does not move anymore.

\subsubsection{Their application in resource allocation}

In the scope of containers assignment on a set of hosts, the bins are the
different servers and the items are the services we want to host. Some
additional aspects have to be considered: applications need different resources
like memory, CPU, persistent storage (disk), network input/output. Often, the
items are multidimensional, and it is named multidimensional vector bin
packing. Moving a VM or container from one host to another has a cost which may
be important for virtual machines, so the number of parallel migrations may be
limited.

In \textit{Adaptive Resource Provisioning for the Cloud Using Online Bin
Packing}~\cite*{reassignmentVisbp}, the authors consider first, that a virtual
machine only has one dimension, its CPU consumption. They reject “strict”
online bin packing, because we often don't know exactly the future consumption
of a virtual machine, so it is necessary to move it afterward, when we can
measure it. Moreover, as VMs can be migrated easily, there is no reason not
considering it if the resulting performance is better. “Relaxed online bin
packing” allows movements when a new item is packed, but an item cannot be
resized. “Dynamic online bin packing” is thought inadequate in this context
too, but often, when an virtual machine has to move the best solution is not
always to remove it then repack it, but to move others instances which are
easier to move.

This is why in~\cite{reassignmentVisbp}, they decided to build an online bin
packing algorithm which suits virtualised environments: “Variable Item Size Bin
Packing”, its characteristics are the following.

\begin{itemize}
	\item{As relaxed online bin packing, it allows movements when a new item is packed}
	\item{Stronger limit of movements, to avoid executing too many migrations}
	\item{A \texttt{change} operation is defined to modify the size of an item in a bin}
\end{itemize}

They extend their algorithm to multidimensional vectors by considering the biggest
value among the different dimensions of a vector, so the problem returns to
one-dimension. Using this way to simplify the problem is working in some cases.
Commonly when a resource consumption increases the others are following. For
example an application having a high network bandwidth requirement, would also have a
high CPU consumption. Finally, they admit that this solution would work quite poorly
in the case of instances with non-proportional requirements.

In~\cite{allocationHeterogeneous}, we have seen that the first approach of the author
was around linear programming, but the main part of their work is defining a way
to apply multidimensional vector bin packing to heterogeneous environments. On a first side,
they deal with the multidimensional aspect of this problem. It is necessary to specify how to
sort the items because there is not natural way to sort these vector.

\begin{itemize}
	\item Value of the maximal dimension
	\item Sum of all dimensions
	\item Ratio of the max/min
	\item Difference max-min
	\item Lexicographic order
	\item None
\end{itemize}

Most of the previous algorithms are not considering the way the bins are used.
In this publication, as it is targeting heterogeneous infrastructure, the order
the bins are sorted when executing any algorithm matters. All the previous way to
sort the items can be applied to the set of bins.

All these previous possibilities of ordering among the virtual machines and physical
hosts are combined and result in a “meta” algorithm (METAHVP) which takes the best result
out of the different combinations of one items ordering and one bins ordering. After
individual analysis, some sort types are removed from the meta algorithm to improve
its runtime. (METAHVPLIGHT)

The simulation achieved to test these heuristics are comparing the results to those
which have been found using the linear programming method and those obtained using
greedy algorithms (*-Fit). The conclusion is that METAHVP has the best results over
all the other, and METAHVPLIGHT achieves this result in on tenth of METAHVP's runtime.

Finally, according to what we want to study there are several possible solutions using
bin packing. Semantically, it is really comfortable to compare bins with physical servers
and items with virtual machines, it allows a very natural vision of this problem.

\subsection{Others}

To deal with mathematical optimization and approximative solution of NP-complete problems,
Ants colony algorithms, genetic algorithms and some other famous methods, based on statistical
analysis.

\subsubsection{Ant colony algorithms}

In~\cite{algoAntcolony1, algoAntcolony2, algoAntcolony3}, the ant colony
algorithms are studied. As we can see in the following graph:

\begin{figure}[H]
\begin{center}
	\includegraphics[width=0.8\textwidth]{./Images/antcolonyperf.png}
	\caption{Comparison between First Fit Decreased and Ant Colony algorithms in~\cite{algoAntcolony2}}
\end{center}
\end{figure}

The simulation shows that the ant colony gets better performance than a simple greedy First
Fit Decreasing, however this improvement is not free:

\begin{figure}[H]
\begin{center}
	\includegraphics[width=0.6\textwidth]{./Images/antcolonyruntime.png}
	\caption{Runtime of First Fit Decreased and Ant Colony algorithms in~\cite{algoAntcolony2}}
\end{center}
\end{figure}

When the number of nodes becomes bigger, the time spent to find the optimal
allocation grows hugely, it is thousands times longer than a simple First Fit
Decreasing for 3 to 5 percents of improvement. For analysis purpose it is
something interesting to get better results, but in a realistic point of view,
this operation can not take several hours as it should be repeated often.

\subsubsection{Genetic algorithms}

Genetic algorithms (GA) are heuristics based on natural selection. Generations
of solutions are mutating, inheriting with and from each other to result in
close to optimal results.~\cite{algoGenetics1} and~\cite{algoGenetics2}
focused on them to solve the virtual machines assignment problem. In the work
of David Wilcox et al.\cite{algoGenetics1}, simulations are comparing GA with
*-Fit algorithms.

\begin{figure}[b!]
\begin{center}
	\includegraphics[width=0.65\textwidth]{./Images/geneticperf1.png}
	\caption{Results of simulations using a genetic algorithm\cite{algoGenetics1}}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
	\includegraphics[width=0.65\textwidth]{./Images/geneticperf2.png}
	\caption{Results of simulations using a genetic algorithm\cite{algoGenetics1}}
\end{center}
\end{figure}

On the following graphs, ICDF stands for “Inverse Cumulative Distribution
Function” also known as “quantile function”, the authors use it to represent
the load: “Using the icdf, we can specify a percentile value and obtain a
corresponding load which can be passed to the assignment algorithm”.

The conclusion which is that GA tends to consume less physical hosts, at any
load, the number of PMs is largely under the amount of servers used by the
other bin packing algorithms. As a direct consequence, the PMs which are
over-capacitated (where the amount of VMs exceed the resource capacity of the
physical sever), is much more high. For this reason, this approach can hardly
be used in environment where a SLA (Service Level Agreement) has to be
respected, because if there are overloaded servers, some applications or tasks
running of them will be slowed by this situation.

\subsubsection{Network flows}

Network flows are basically directed graphs where each edge has a capacity and a flow.
The main property is that each node of this graph must have an equal sum of flows from
the edges directed to it and leaving from it, except for two particular type of nodes:
“the source node” and “the sink node”.

\begin{figure}[H]
\begin{center}
	\includegraphics[width=0.8\textwidth]{./Images/examplenetwork.png}
	\caption{Example of network flow directed graph}
\end{center}
\end{figure}

Some people have used this concept to build a model to solve the resource
allocation problem, to fine a close to optimal solution. Kimish Patel, Murali
Annavaram and Massoud Pedram worked on resource assignment in
datacenter\cite{allocationNetworkflow}, considering an heterogeneous
environment as in~\cite{allocationHeterogeneous}. Each set of similar servers,
considered as a pool of servers is represented by a node, with a capacity
different from each other according to the differences between two pools of
servers.

Unfortunately, this technique does not seem to be used for virtual machines
allocation, and the link between this method and the problem we are dealing
with is not obvious at all.

\section{Real data analysis}

Most of the cited works in the literature review are basing their work on
simulations. In the experiments, simulation tools like
SimGrid\cite{websiteSimgrid} or CloudSim\cite{websiteCloudsim} are used to
simulate the behavior of one or multiple cloud infrastructures.

The data may be generated randomly or following some statistical rules, but
often, workloads are based of extract of real workload. Typically, Google is
releasing workloads of its own production infrastructure. 

In 2012, Google sponsored the ROADEF contest (Operational Research and Decision
Support French Society)\cite{websiteRoadef}. The contest was focusing the machine
reassignment problem based on Google workload. Each attendee had to find the
best solution. Some of them resulted in an official publication
like “Heuristics and matheuristics for a real-life machine reassignment
problem”~\cite*{roadefIp}.  They based their work on linear programming.
However in~\cite{roadefBp1, roadefBp2}, the authors have used
around the bin packing algorithms. Unfortunately, the work of the winner has
not been published so we are not able to see which algorithm has been used to
achieve the best reassignment.
